from smolagents import OpenAIServerModel, CodeAgent, ToolCallingAgent, HfApiModel, tool, GradioUI
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
import os

load_dotenv()

# Environment variables
reasoning_model_id = os.getenv("REASONING_MODEL_ID")  # e.g., "o3-mini"
tool_model_id = os.getenv("TOOL_MODEL_ID")
huggingface_api_token = os.getenv("HUGGINGFACE_API_TOKEN")
openai_api_key = os.getenv("OPENAI_API_KEY")

def get_reasoning_model(model_id):
    """
    Always load the reasoning model using OpenAI's API.
    """
    return OpenAIServerModel(
        model_id=model_id,
        api_base="https://api.openai.com/v1",
        api_key=openai_api_key
    )

def get_model(model_id):
    """
    Returns a model instance via HuggingFace.
    This is used for the tool model.
    """
    return HfApiModel(model_id=model_id, token=huggingface_api_token)

# Load the reasoning model using OpenAI API (for example, o3-mini)
reasoning_model = get_reasoning_model(reasoning_model_id)
reasoner = CodeAgent(tools=[], model=reasoning_model, add_base_tools=False, max_steps=2)

# Initialize vector store and embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={'device': 'cpu'}
)
db_dir = os.path.join(os.path.dirname(__file__), "chroma_db")
vectordb = Chroma(persist_directory=db_dir, embedding_function=embeddings)

@tool
def rag_with_reasoner(user_query: str) -> str:
    """
    Retrieves relevant document chunks from the vector database based on the user's query
    and uses the reasoning model to generate a concise answer.

    Parameters:
        user_query (str): The question or query provided by the user.

    Returns:
        str: A concise answer generated by the reasoning model using the retrieved context.
    """
    # Search for relevant documents
    docs = vectordb.similarity_search(user_query, k=3)
    
    # Combine document contents into one context string
    context = "\n\n".join(doc.page_content for doc in docs)
    
    # Create prompt including context and the user's question
    prompt = f"""Based on the following context, answer the user's question. Be concise and specific.
If there isn't sufficient information, provide a better query for RAG.

Context:
{context}

Question: {user_query}

Answer:"""
    
    # Get response from the reasoning model
    response = reasoner.run(prompt, reset=False)
    return response

# Load the tool model (remains unchanged)
tool_model = get_model(tool_model_id)
primary_agent = ToolCallingAgent(tools=[rag_with_reasoner], model=tool_model, add_base_tools=False, max_steps=3)

def main():
    GradioUI(primary_agent).launch()

if __name__ == "__main__":
    main()
